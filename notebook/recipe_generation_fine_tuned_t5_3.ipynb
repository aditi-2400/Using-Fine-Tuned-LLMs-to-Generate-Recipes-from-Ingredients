{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1728879,"sourceType":"datasetVersion","datasetId":1025978}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets sentencepiece","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets rouge_score sacrebleu evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:57:18.358091Z","iopub.execute_input":"2025-05-12T18:57:18.358544Z","iopub.status.idle":"2025-05-12T18:57:25.311456Z","shell.execute_reply.started":"2025-05-12T18:57:18.358517Z","shell.execute_reply":"2025-05-12T18:57:25.310657Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=fad2663bfac4d12400a7209ec929d9df871f542a20221db01fc783a35a13968a\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: portalocker, fsspec, sacrebleu, rouge_score, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0 portalocker-3.1.1 rouge_score-0.1.2 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, KFold\nfrom transformers import T5Tokenizer\nfrom transformers import T5ForConditionalGeneration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:45:30.651014Z","iopub.execute_input":"2025-05-12T15:45:30.651731Z","iopub.status.idle":"2025-05-12T15:45:30.656370Z","shell.execute_reply.started":"2025-05-12T15:45:30.651707Z","shell.execute_reply":"2025-05-12T15:45:30.655550Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dataset_path = '/kaggle/input/recipenlg/RecipeNLG_dataset.csv'\ndf = pd.read_csv(dataset_path)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:45:35.021565Z","iopub.execute_input":"2025-05-12T15:45:35.022230Z","iopub.status.idle":"2025-05-12T15:46:00.891333Z","shell.execute_reply.started":"2025-05-12T15:45:35.022196Z","shell.execute_reply":"2025-05-12T15:46:00.890705Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                  title  \\\n0           0    No-Bake Nut Cookies   \n1           1  Jewell Ball'S Chicken   \n2           2            Creamy Corn   \n3           3          Chicken Funny   \n4           4   Reeses Cups(Candy)     \n\n                                         ingredients  \\\n0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n1  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n2  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n3  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n4  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n\n                                          directions  \\\n0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n1  [\"Place chipped beef on bottom of baking dish....   \n2  [\"In a slow cooker, combine all ingredients. C...   \n3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n4  [\"Combine first four ingredients and press in ...   \n\n                                              link    source  \\\n0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n\n                                                 NER  \n0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>ingredients</th>\n      <th>directions</th>\n      <th>link</th>\n      <th>source</th>\n      <th>NER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>No-Bake Nut Cookies</td>\n      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n      <td>Gathered</td>\n      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Jewell Ball'S Chicken</td>\n      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n      <td>[\"Place chipped beef on bottom of baking dish....</td>\n      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n      <td>Gathered</td>\n      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Creamy Corn</td>\n      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n      <td>Gathered</td>\n      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Chicken Funny</td>\n      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n      <td>Gathered</td>\n      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Reeses Cups(Candy)</td>\n      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n      <td>[\"Combine first four ingredients and press in ...</td>\n      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n      <td>Gathered</td>\n      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Data Sampling","metadata":{}},{"cell_type":"code","source":"# Randomly sample 10,000 rows from the train dataset\nsampled_data = df.sample(n=50000, random_state=42)\n\n# Save the sampled dataset\nsampled_data.to_csv(\"sampled_train.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:08.633101Z","iopub.execute_input":"2025-05-12T15:46:08.633365Z","iopub.status.idle":"2025-05-12T15:46:10.256872Z","shell.execute_reply.started":"2025-05-12T15:46:08.633345Z","shell.execute_reply":"2025-05-12T15:46:10.256073Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#compare stringified lists to python lists\nimport ast\nsampled_data['ingredients'] = sampled_data['ingredients'].apply(ast.literal_eval)\nsampled_data['directions'] = sampled_data['directions'].apply(ast.literal_eval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:13.892951Z","iopub.execute_input":"2025-05-12T15:46:13.893826Z","iopub.status.idle":"2025-05-12T15:46:15.844065Z","shell.execute_reply.started":"2025-05-12T15:46:13.893787Z","shell.execute_reply":"2025-05-12T15:46:15.843464Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#create input output pairs\nsampled_data['input'] = sampled_data['ingredients'].apply(lambda x: \"Ingredients: \" + \", \".join(x))\nsampled_data['output'] = sampled_data['directions'].apply(lambda x: \" \".join(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:20.342906Z","iopub.execute_input":"2025-05-12T15:46:20.343570Z","iopub.status.idle":"2025-05-12T15:46:20.413571Z","shell.execute_reply.started":"2025-05-12T15:46:20.343548Z","shell.execute_reply":"2025-05-12T15:46:20.412844Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from datasets import Dataset\nfull_dataset = Dataset.from_pandas(sampled_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:23.041852Z","iopub.execute_input":"2025-05-12T15:46:23.042389Z","iopub.status.idle":"2025-05-12T15:46:24.527627Z","shell.execute_reply.started":"2025-05-12T15:46:23.042368Z","shell.execute_reply":"2025-05-12T15:46:24.526871Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_val_df, test_df = train_test_split(sampled_data,test_size=0.1,random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:27.365317Z","iopub.execute_input":"2025-05-12T15:46:27.365886Z","iopub.status.idle":"2025-05-12T15:46:27.396604Z","shell.execute_reply.started":"2025-05-12T15:46:27.365855Z","shell.execute_reply":"2025-05-12T15:46:27.395729Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Convert train and test DataFrames to Hugging Face Dataset\ntrain_val_dataset = Dataset.from_pandas(train_val_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:29.564279Z","iopub.execute_input":"2025-05-12T15:46:29.564547Z","iopub.status.idle":"2025-05-12T15:46:30.302340Z","shell.execute_reply.started":"2025-05-12T15:46:29.564528Z","shell.execute_reply":"2025-05-12T15:46:30.301783Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"#tokenizing the dataset\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:34.033216Z","iopub.execute_input":"2025-05-12T15:46:34.033481Z","iopub.status.idle":"2025-05-12T15:46:35.448649Z","shell.execute_reply.started":"2025-05-12T15:46:34.033463Z","shell.execute_reply":"2025-05-12T15:46:35.447855Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b121e5aa121a458c8c6edee99568091f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21aa700ebaa3401580ea383bd7ab36e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c489e08b92d4dbb88a75fd6f08447e9"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def tokenize_batch(batch):\n  # input_text = batch['input']\n  # output_text = batch['output']\n  tokenized_input = tokenizer(batch['input'], padding='max_length',truncation=True, max_length=512)\n  tokenized_output = tokenizer(batch['output'], padding='max_length',truncation=True, max_length=512)\n  tokenized_input['labels'] = tokenized_output['input_ids']\n  return tokenized_input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:42.026707Z","iopub.execute_input":"2025-05-12T15:46:42.027400Z","iopub.status.idle":"2025-05-12T15:46:42.031312Z","shell.execute_reply.started":"2025-05-12T15:46:42.027377Z","shell.execute_reply":"2025-05-12T15:46:42.030565Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# train_tokenized = train_dataset.map(tokenize_batch, batched=True)\n# test_tokenized = test_dataset.map(tokenize_batch, batched=True)\n\ntrain_val_tokenized = train_val_dataset.map(tokenize_batch, batched=True)\ntest_tokenized = test_dataset.map(tokenize_batch, batched=True)\n\n#tokenized_dataset.save_to_disk(\"tokenized_full_dataset\")\ntrain_val_tokenized.save_to_disk(\"tokenized_train_val_dataset\")\ntest_tokenized.save_to_disk(\"tokenized_test_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:46:45.295091Z","iopub.execute_input":"2025-05-12T15:46:45.295785Z","iopub.status.idle":"2025-05-12T15:47:32.609220Z","shell.execute_reply.started":"2025-05-12T15:46:45.295756Z","shell.execute_reply":"2025-05-12T15:47:32.608427Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65b7029874d6432fb5d301aaa93c489e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8f5b1ea8ef2433aabab13046d608441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/45000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a262d36c37324b6bad1de3fdf5d84d39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18bc864d68d946a5b972a9da4e17a527"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# print(train_dataset.column_names)\n# print(test_dataset.column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_from_disk\n\n# Load tokenized dataset\ntrain_val_tokenized = load_from_disk(\"tokenized_train_val_dataset\")\ntest_tokenized = load_from_disk(\"tokenized_test_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:47:41.795734Z","iopub.execute_input":"2025-05-12T15:47:41.796058Z","iopub.status.idle":"2025-05-12T15:47:41.862315Z","shell.execute_reply.started":"2025-05-12T15:47:41.796037Z","shell.execute_reply":"2025-05-12T15:47:41.861614Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Hyperparameter ranges for fine tuning","metadata":{}},{"cell_type":"code","source":"# Hyperparameter ranges\nlearning_rates = [1e-5, 1e-4]\nnum_epochs_list = [5]\nbatch_size = 8\nl2_regularizations = [0.0, 1e-4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:47:45.393118Z","iopub.execute_input":"2025-05-12T15:47:45.393582Z","iopub.status.idle":"2025-05-12T15:47:45.397417Z","shell.execute_reply.started":"2025-05-12T15:47:45.393559Z","shell.execute_reply":"2025-05-12T15:47:45.396600Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Cross Validation for Hyperparameter selection","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=3, shuffle=True, random_state=42)\nresults = []\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:47:47.876443Z","iopub.execute_input":"2025-05-12T15:47:47.877208Z","iopub.status.idle":"2025-05-12T15:47:47.881004Z","shell.execute_reply.started":"2025-05-12T15:47:47.877182Z","shell.execute_reply":"2025-05-12T15:47:47.880219Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Fine tuning the t5 model","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    # Ensure every element in batch is converted to tensor, but ignore strings (such as tokenized words)\n    #print(f\"First batch item: {batch[0]}\")\n    #return {key: torch.tensor([d[key] for d in batch if isinstance(d[key], (list, int))]) for key in batch[0]}\n    return {\n        'input_ids': torch.tensor([d['input_ids'] for d in batch]),\n        'attention_mask': torch.tensor([d['attention_mask'] for d in batch]),\n        'labels': torch.tensor([d['labels'] for d in batch]),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:47:52.692471Z","iopub.execute_input":"2025-05-12T15:47:52.693065Z","iopub.status.idle":"2025-05-12T15:47:52.697239Z","shell.execute_reply.started":"2025-05-12T15:47:52.693042Z","shell.execute_reply":"2025-05-12T15:47:52.696572Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"best_val_loss = float('inf')\nbest_state_dict = None\nbest_hparams = None \n\nfor lr in learning_rates:\n    for num_epochs in num_epochs_list:\n        for l2_reg in l2_regularizations:\n            fold = 0\n            fold_losses = []\n\n            # Perform cross-validation\n            for train_index, val_index in kf.split(train_val_tokenized):\n                fold += 1\n                print(f\"\\nTraining fold {fold} with lr={lr}, epochs={num_epochs}, l2_reg={l2_reg}\")\n\n                train_subset = train_val_tokenized.select(train_index)\n                val_subset = train_val_tokenized.select(val_index)\n\n                train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n                \n                val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n                model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n                model.to(device)\n\n                # Optimizer with L2 regularization\n                optimizer = torch.optim.AdamW(\n                    model.parameters(), \n                    lr=lr, \n                    weight_decay=l2_reg\n                )\n                \n                # Training loop\n                for epoch in range(num_epochs):\n                    model.train()\n                    running_loss = 0.0\n                    for batch in train_dataloader:\n                        optimizer.zero_grad()\n                        batch = {key: val.to(device) for key, val in batch.items()}\n                        outputs = model(\n                            input_ids=batch['input_ids'],\n                            attention_mask=batch['attention_mask'],\n                            labels=batch['labels']\n                        )\n                        loss = outputs.loss\n                        loss.backward()\n                        optimizer.step()\n                        running_loss += loss.item()\n                    \n                    avg_train_loss = running_loss / len(train_dataloader)\n                    print(f\"Epoch {epoch+1}/{num_epochs} => Train Loss: {avg_train_loss:.4f}\")\n\n                # Validation\n                model.eval()\n                val_loss = 0.0\n                for batch in val_dataloader:\n                    with torch.no_grad():\n                        batch = {key: val.to(device) for key, val in batch.items()}\n                        outputs = model(\n                            input_ids=batch['input_ids'],\n                            attention_mask=batch['attention_mask'],\n                            labels=batch['labels']\n                        )\n                        val_loss += outputs.loss.item()\n                \n                avg_val_loss = val_loss / len(val_dataloader)\n                print(f\"Fold {fold} Validation Loss: {avg_val_loss:.4f}\")\n                fold_losses.append(avg_val_loss)\n            \n            # Average validation loss for this hyperparameter set\n            mean_val_loss = np.mean(fold_losses)\n            print(f\"\\nMean Validation Loss for lr={lr}, epochs={num_epochs}, l2_reg={l2_reg}: {mean_val_loss:.4f}\")\n            if mean_val_loss < best_val_loss:\n                best_val_loss = mean_val_loss\n                best_hparams = (lr, num_epochs, l2_reg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:28:52.828283Z","iopub.execute_input":"2025-05-11T18:28:52.828609Z","execution_failed":"2025-05-12T06:23:55.487Z"}},"outputs":[{"name":"stdout","text":"\nTraining fold 1 with lr=1e-05, epochs=5, l2_reg=0.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a14c1ecbbcf94622a277ab2814327b47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770e28db81564c87a89d6e0fda91c048"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffedcd909b8248b3a4ffa6f072ce49fe"}},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 => Train Loss: 1.3834\nEpoch 2/5 => Train Loss: 0.7885\nEpoch 3/5 => Train Loss: 0.7433\nEpoch 4/5 => Train Loss: 0.7196\nEpoch 5/5 => Train Loss: 0.7032\nFold 1 Validation Loss: 0.6456\n\nTraining fold 2 with lr=1e-05, epochs=5, l2_reg=0.0\nEpoch 1/5 => Train Loss: 1.3824\nEpoch 2/5 => Train Loss: 0.7933\nEpoch 3/5 => Train Loss: 0.7496\nEpoch 4/5 => Train Loss: 0.7250\nEpoch 5/5 => Train Loss: 0.7087\nFold 2 Validation Loss: 0.6382\n\nTraining fold 3 with lr=1e-05, epochs=5, l2_reg=0.0\nEpoch 1/5 => Train Loss: 1.3861\nEpoch 2/5 => Train Loss: 0.7872\nEpoch 3/5 => Train Loss: 0.7431\nEpoch 4/5 => Train Loss: 0.7199\nEpoch 5/5 => Train Loss: 0.7038\nFold 3 Validation Loss: 0.6443\n\nMean Validation Loss for lr=1e-05, epochs=5, l2_reg=0.0: 0.6427\n\nTraining fold 1 with lr=1e-05, epochs=5, l2_reg=0.0001\nEpoch 1/5 => Train Loss: 1.4026\nEpoch 2/5 => Train Loss: 0.7891\nEpoch 3/5 => Train Loss: 0.7435\nEpoch 4/5 => Train Loss: 0.7195\nEpoch 5/5 => Train Loss: 0.7033\nFold 1 Validation Loss: 0.6461\n\nTraining fold 2 with lr=1e-05, epochs=5, l2_reg=0.0001\nEpoch 1/5 => Train Loss: 1.3815\nEpoch 2/5 => Train Loss: 0.7938\nEpoch 3/5 => Train Loss: 0.7493\nEpoch 4/5 => Train Loss: 0.7253\nEpoch 5/5 => Train Loss: 0.7089\nFold 2 Validation Loss: 0.6386\n\nTraining fold 3 with lr=1e-05, epochs=5, l2_reg=0.0001\nEpoch 1/5 => Train Loss: 1.4024\nEpoch 2/5 => Train Loss: 0.7917\nEpoch 3/5 => Train Loss: 0.7445\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Final training with the best set of coefficients","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/fine_tuned_t5\")\ntokenizer.save_pretrained(\"/kaggle/working/fine_tuned_t5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T07:12:04.922578Z","iopub.execute_input":"2025-05-12T07:12:04.922966Z","iopub.status.idle":"2025-05-12T07:12:05.506672Z","shell.execute_reply.started":"2025-05-12T07:12:04.922940Z","shell.execute_reply":"2025-05-12T07:12:05.505817Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/fine_tuned_t5/tokenizer_config.json',\n '/kaggle/working/fine_tuned_t5/special_tokens_map.json',\n '/kaggle/working/fine_tuned_t5/spiece.model',\n '/kaggle/working/fine_tuned_t5/added_tokens.json')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"best_hparams = (1e-04, 5, 0.0001)\nlr_best, epochs_best, l2_best = best_hparams\nfinal_dl = DataLoader(train_val_tokenized, batch_size=batch_size,\n                      shuffle=True, collate_fn=collate_fn)\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr_best, weight_decay=l2_best)\n\nfor epoch in range(epochs_best):\n    model.train(); running = 0\n    for batch in final_dl:\n        optimizer.zero_grad()\n        batch = {k:v.to(device) for k,v in batch.items()}\n        loss = model(**batch).loss\n        loss.backward(); optimizer.step()\n        running += loss.item()\n    print(f\"final‑train epoch {epoch+1}/{epochs_best} loss {running/len(final_dl):.4f}\")\n\n# save final model ONCE\nmodel.save_pretrained(\"/kaggle/working/fine_tuned_t5_2\")\ntokenizer.save_pretrained(\"/kaggle/working/fine_tuned_t5_2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T15:48:19.404646Z","iopub.execute_input":"2025-05-12T15:48:19.404951Z","iopub.status.idle":"2025-05-12T18:50:11.064406Z","shell.execute_reply.started":"2025-05-12T15:48:19.404929Z","shell.execute_reply":"2025-05-12T18:50:11.063733Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87738b959cc34b44b2727d397a277d34"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f3d8bc60e1840f2926947f61521eb1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"165f824d2fdf4f048dc2de4b76a29703"}},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"final‑train epoch 1/5 loss 0.7615\nfinal‑train epoch 2/5 loss 0.6341\nfinal‑train epoch 3/5 loss 0.6006\nfinal‑train epoch 4/5 loss 0.5794\nfinal‑train epoch 5/5 loss 0.5640\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/fine_tuned_t5_2/tokenizer_config.json',\n '/kaggle/working/fine_tuned_t5_2/special_tokens_map.json',\n '/kaggle/working/fine_tuned_t5_2/spiece.model',\n '/kaggle/working/fine_tuned_t5_2/added_tokens.json')"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"# Generating Recipes","metadata":{}},{"cell_type":"code","source":"def generate_recipe(ingredients):\n    input_text = f\"Ingredients: { ', '.join(ingredients)}\"\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n    outputs = model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:45.030534Z","iopub.execute_input":"2025-05-12T18:52:45.031084Z","iopub.status.idle":"2025-05-12T18:52:45.035029Z","shell.execute_reply.started":"2025-05-12T18:52:45.031060Z","shell.execute_reply":"2025-05-12T18:52:45.034245Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"ingredients = ['1 dozen eggs', '1 cup flour', '1 gallon milk', '1 cup sugar', '1 oz baking powder']\nprint(generate_recipe(ingredients))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:31.436592Z","iopub.execute_input":"2025-05-12T18:53:31.437165Z","iopub.status.idle":"2025-05-12T18:53:32.214712Z","shell.execute_reply.started":"2025-05-12T18:53:31.437144Z","shell.execute_reply":"2025-05-12T18:53:32.213895Z"}},"outputs":[{"name":"stdout","text":"Preheat oven to 350 degrees F. Grease and flour a 9x13 baking pan. In a large mixing bowl, mix together the eggs, flour, milk, sugar, and baking powder. Stir in the egg mixture and mix well. Pour into prepared pan. Bake for 20 minutes or until a toothpick comes out clean.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Evaluating the model","metadata":{}},{"cell_type":"code","source":"#from tdqm import tdqm\ndef model_evaluate(test_dataloader,model,device,tokenizer):\n    model.eval()\n    predictions=[]\n    references=[]\n    for batch in test_dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        reference_texts = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n        with torch.no_grad():\n            outputs = model.generate(input_ids,attention_mask=attention_mask, max_length=512, num_beams=4, early_stopping=True)\n        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        predictions.extend(generated_texts)\n        references.extend(reference_texts)\n    return predictions,references","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:54:34.669084Z","iopub.execute_input":"2025-05-12T18:54:34.669604Z","iopub.status.idle":"2025-05-12T18:54:34.674407Z","shell.execute_reply.started":"2025-05-12T18:54:34.669582Z","shell.execute_reply":"2025-05-12T18:54:34.673637Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import evaluate\nbleu_metric = evaluate.load('sacrebleu')\nrouge_metric = evaluate.load('rouge')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:57:29.206229Z","iopub.execute_input":"2025-05-12T18:57:29.206830Z","iopub.status.idle":"2025-05-12T18:57:31.514058Z","shell.execute_reply.started":"2025-05-12T18:57:29.206795Z","shell.execute_reply":"2025-05-12T18:57:31.513457Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0f81da810fd4fbea1dc518baac909f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8585f8179e05446b8fab2d79f4307694"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"def compute_metrics(predictions,references):\n    bleu_score = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n    rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n    \n    return {\"bleu\": bleu_score['score'], \"rouge\": rouge_result}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:57:35.442562Z","iopub.execute_input":"2025-05-12T18:57:35.443144Z","iopub.status.idle":"2025-05-12T18:57:35.447367Z","shell.execute_reply.started":"2025-05-12T18:57:35.443122Z","shell.execute_reply":"2025-05-12T18:57:35.446765Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"test_dataloader = DataLoader(test_tokenized, batch_size=batch_size, collate_fn=collate_fn)\npredictions,references = model_evaluate(test_dataloader,model,device,tokenizer)\nmetrics = compute_metrics(predictions, references)\nprint(f\"BLEU Score: {metrics['bleu']}\")\nprint(f\"ROUGE Scores: {metrics['rouge']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:57:39.123564Z","iopub.execute_input":"2025-05-12T18:57:39.124255Z","iopub.status.idle":"2025-05-12T19:26:36.061460Z","shell.execute_reply.started":"2025-05-12T18:57:39.124232Z","shell.execute_reply":"2025-05-12T19:26:36.060830Z"}},"outputs":[{"name":"stdout","text":"BLEU Score: 7.249748550179903\nROUGE Scores: {'rouge1': 0.33777885202650104, 'rouge2': 0.11686751416760655, 'rougeL': 0.2467904676888479, 'rougeLsum': 0.24689741359084372}\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Downloading the model","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\nshutil.make_archive('/kaggle/working/fine_tuned_t5_2','zip','/kaggle/working/fine_tuned_t5_2')\nfrom IPython.display import FileLink\n\n# Create a download link for the zipped model\nFileLink(\"/kaggle/working/fine_tuned_t5_2.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:04.898488Z","iopub.execute_input":"2025-05-12T18:52:04.898766Z","iopub.status.idle":"2025-05-12T18:52:17.178262Z","shell.execute_reply.started":"2025-05-12T18:52:04.898747Z","shell.execute_reply":"2025-05-12T18:52:17.177286Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/fine_tuned_t5_2.zip","text/html":"<a href='/kaggle/working/fine_tuned_t5_2.zip' target='_blank'>/kaggle/working/fine_tuned_t5_2.zip</a><br>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}